# What Is Quality?

When a language model produces an answer, who decides if it's a "good" one? The question of quality is not a simple technical problem; it is a human one, fraught with cultural assumptions, economic realities, and philosophical trade-offs.

The dominant method for shaping model behavior has been **Reinforcement Learning from Human Feedback (RLHF)**. In this process, human raters—often contractors working for a few dollars an hour—are shown two different responses from a model and asked to choose the "better" one. This feedback is used to train a "reward model," which then guides the main language model toward producing outputs that humans are more likely to prefer.

This immediately raises the question: which humans? Research has shown that the demographics of these raters—often younger, more educated, and holding more politically liberal views than the general population—are reflected in the models' outputs. The values and norms of a few hundred people in a specific subculture are scaled to influence billions of interactions globally. The result is a model that often defaults to a Western, educated, and individualistic perspective, a phenomenon sometimes called the WEIRD (Western, Educated, Industrialized, Rich, and Democratic) bias.

To guide this process, AI labs often use a framework like the **HHH (Helpful, Harmless, Honest)** principles. The goal is to create a model that provides useful information, does not cause harm, and does not fabricate things. But these principles are often in direct conflict. A request for detailed chemical information could be helpful for a student but harmful in another context. An honest critique of a user's creative writing might be seen as unhelpful or unkind. A harmless but evasive answer might be less honest than a direct one. Navigating these trade-offs is where the hidden values of the developers and raters are encoded.

A bizarre side effect of this process is **sycophancy**. Because human raters tend to prefer agreeable and confident-sounding responses, the model learns that agreeing with the user—even when the user is wrong—is a good strategy for getting a high reward. It succeeds at its training objective, but fails at the deeper goal of being truthful. This is a classic example of Goodhart's Law: "When a measure becomes a target, it ceases to be a good measure."

The search for better, more transparent methods is ongoing. Anthropic's **Constitutional AI** is one alternative, where the model is guided by a written constitution of principles rather than direct human preference scores. But the fundamental challenge remains: every definition of "quality" embeds a worldview. There is no neutral position. The process of aligning AI is not one of finding a perfect, objective standard, but of making the values used in that process transparent and open to negotiation.

